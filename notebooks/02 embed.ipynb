{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Image Embeddings\n",
    "\n",
    "This notebook generates vector embeddings for marketplace images using the DINOv2 (Vision Transformer) model:\n",
    "\n",
    "- **Load DINOv2 model**: Uses the pre-trained `dinov2` model from Facebook Research\n",
    "- **Process images**: Batch-processes all dataset images through the model\n",
    "- **Extract embeddings**: Generates 768-dimensional feature vectors for each image\n",
    "- **Storage**: Embeddings are stored in a PostgreSQL database with vector search extension\n",
    "\n",
    "These embeddings enable image similarity search and retrieval in the marketplace image RAG system.\n",
    "\n",
    "The code from this notebook is refined and integrated into the production codebase, where embeddings are persisted to PostgreSQL with HNSW index for scalable vector search operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "notebooks_dir = Path().absolute()\n",
    "project_dir = notebooks_dir.parent\n",
    "os.chdir(project_dir)\n",
    "load_dotenv()\n",
    "sys.path.append(project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from marketplace.const import DATA_ROOT\n",
    "from marketplace.dataset import MarketplaceDataModule\n",
    "from marketplace.db import ImageEmbedding, save_image_embeddings\n",
    "from marketplace.model import DinoV2WithNormalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DinoV2WithNormalize(\n",
       "  (model): DinoVisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x NestedTensorBlock(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): MemEffAttention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"xFormers is available\")\n",
    "model = DinoV2WithNormalize()\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with First Batch\n",
    "\n",
    "Test the embedding generation with just the first batch to verify everything works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([4, 3, 224, 224])\n",
      "Embeddings shape: torch.Size([4, 768])\n",
      "\n",
      "data/adults handicrafts/261226323.jpg\n",
      "Embedding (first 10 values): tensor([ 0.0478,  0.0214,  0.0208, -0.0664,  0.0213,  0.0381, -0.1012, -0.0127,\n",
      "        -0.0391,  0.0240])\n",
      "Embedding norm: 1.0000\n",
      "\n",
      "data/adults handicrafts/261227883.jpg\n",
      "Embedding (first 10 values): tensor([-0.0039,  0.0138, -0.0275, -0.0247, -0.0110, -0.0788, -0.0222, -0.0807,\n",
      "        -0.0557,  0.0400])\n",
      "Embedding norm: 1.0000\n",
      "\n",
      "data/adults handicrafts/261233434.jpg\n",
      "Embedding (first 10 values): tensor([ 0.0139,  0.0183,  0.0140, -0.0045, -0.0407, -0.0198, -0.0852, -0.0207,\n",
      "         0.0271, -0.0188])\n",
      "Embedding norm: 1.0000\n",
      "\n",
      "data/adults handicrafts/261246318.jpg\n",
      "Embedding (first 10 values): tensor([-0.0027,  0.0496,  0.0160, -0.0441, -0.0478,  0.0509, -0.0549, -0.0247,\n",
      "        -0.0163,  0.0036])\n",
      "Embedding norm: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "data_module = MarketplaceDataModule(batch_size=4, num_workers=1)\n",
    "data_module.prepare_data()\n",
    "data_module.setup(stage=\"predict\")\n",
    "\n",
    "# Get first batch\n",
    "predict_loader = data_module.predict_dataloader()\n",
    "batch_images, batch_labels = next(iter(predict_loader))\n",
    "print(f\"Batch shape: {batch_images.shape}\") # Should be (4, 3, 224, 224)\n",
    "\n",
    "# Generate embeddings for first batch\n",
    "with torch.no_grad():\n",
    "    batch_images = batch_images.to(device)\n",
    "    embeddings = model(batch_images)\n",
    "    \n",
    "print(f\"Embeddings shape: {embeddings.shape}\")  # Should be (4, 768)\n",
    "\n",
    "# Print first batch results\n",
    "for i in range(len(embeddings)):\n",
    "    image_path = predict_loader.dataset.samples[i][0]\n",
    "    embedding = embeddings[i].cpu()\n",
    "    print(f\"\\n{image_path}\")\n",
    "    print(f\"Embedding (first 10 values): {embedding[:10]}\")\n",
    "    print(f\"Embedding norm: {embedding.norm().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Embeddings Pipeline\n",
    "\n",
    "Compute embeddings for all images in the dataset and persist to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images to process: 138969\n",
      "Batch size: 128\n",
      "Number of batches: 1086\n",
      "\n",
      "Generating embeddings...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1086/1086 [07:41<00:00,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed! Processed 138969 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "batch_size = 128\n",
    "data_module = MarketplaceDataModule(batch_size=batch_size, num_workers=8)\n",
    "data_module.prepare_data()\n",
    "data_module.setup(stage=\"predict\")\n",
    "\n",
    "# Get all images\n",
    "predict_loader = data_module.predict_dataloader()\n",
    "\n",
    "print(f\"Total images to process: {len(predict_loader.dataset)}\")\n",
    "print(f\"Batch size: {predict_loader.batch_size}\")\n",
    "print(f\"Number of batches: {len(predict_loader)}\")\n",
    "print(\"\\nGenerating embeddings...\\n\")\n",
    "\n",
    "# Iterate all batches and generate embeddings\n",
    "global_idx = 0\n",
    "with torch.no_grad():\n",
    "    for batch_images, _batch_labels in tqdm(predict_loader, total=len(predict_loader)):\n",
    "        # Move batch to device\n",
    "        batch_images = batch_images.to(device)  # noqa: PLW2901 (redefined-loop-name)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = model(batch_images)\n",
    "        \n",
    "        # Get image paths for this batch\n",
    "        batch_size = batch_images.shape[0]\n",
    "        batch_indices = range(global_idx, global_idx + batch_size)\n",
    "        image_paths = [predict_loader.dataset.samples[i][0] for i in batch_indices]\n",
    "        \n",
    "        # save image path with its embedding to database\n",
    "        db_objects = [\n",
    "            ImageEmbedding(image_path=rel_path, embedding=embedding.cpu())\n",
    "            for path, embedding in zip(image_paths, embeddings, strict=True)\n",
    "            if (rel_path := str(Path(path).relative_to(DATA_ROOT)))\n",
    "        ]\n",
    "        save_image_embeddings(db_objects)\n",
    "        \n",
    "        global_idx += batch_size\n",
    "\n",
    "print(f\"\\nCompleted! Processed {global_idx} images.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marketplace-image-rag (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
